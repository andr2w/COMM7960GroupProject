<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Related Works | COMM7960 AI for Interactive Media Design Group Project - Yifeng LUO</title>
<meta name=keywords content><meta name=description content="To detect fake news videos, a better approach is to first understand how the fake news video is produced. Existing literature [1] analysis fake news videos and detect fake news videos from three levels:
Signal level, Semantic level, Intent level. Signal Level Fake news videos often contain manipulated or generated video and audio content. The manipulated or generated video and audio content refers to the following two operations:
Editing: visual alterations on existing data of video and audio modality."><meta name=author content="Me"><link rel=canonical href=https://andr2w.github.io/COMM7960GroupProject/related-works/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/COMM7960GroupProject/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://andr2w.github.io/COMM7960GroupProject/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://andr2w.github.io/COMM7960GroupProject/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://andr2w.github.io/COMM7960GroupProject/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://andr2w.github.io/COMM7960GroupProject/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://andr2w.github.io/COMM7960GroupProject/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://andr2w.github.io/COMM7960GroupProject/related-works/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Related Works"><meta property="og:description" content="To detect fake news videos, a better approach is to first understand how the fake news video is produced. Existing literature [1] analysis fake news videos and detect fake news videos from three levels:
Signal level, Semantic level, Intent level. Signal Level Fake news videos often contain manipulated or generated video and audio content. The manipulated or generated video and audio content refers to the following two operations:
Editing: visual alterations on existing data of video and audio modality."><meta property="og:type" content="article"><meta property="og:url" content="https://andr2w.github.io/COMM7960GroupProject/related-works/"><meta property="og:image" content="https://andr2w.github.io/COMM7960GroupProject/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content><meta property="og:site_name" content="COMM7960 AI for Interactive Media Design Group Project - Yifeng LUO"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://andr2w.github.io/COMM7960GroupProject/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Related Works"><meta name=twitter:description content="To detect fake news videos, a better approach is to first understand how the fake news video is produced. Existing literature [1] analysis fake news videos and detect fake news videos from three levels:
Signal level, Semantic level, Intent level. Signal Level Fake news videos often contain manipulated or generated video and audio content. The manipulated or generated video and audio content refers to the following two operations:
Editing: visual alterations on existing data of video and audio modality."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Related Works","item":"https://andr2w.github.io/COMM7960GroupProject/related-works/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Related Works","name":"Related Works","description":"To detect fake news videos, a better approach is to first understand how the fake news video is produced. Existing literature [1] analysis fake news videos and detect fake news videos from three levels:\nSignal level, Semantic level, Intent level. Signal Level Fake news videos often contain manipulated or generated video and audio content. The manipulated or generated video and audio content refers to the following two operations:\nEditing: visual alterations on existing data of video and audio modality.","keywords":[],"articleBody":"To detect fake news videos, a better approach is to first understand how the fake news video is produced. Existing literature [1] analysis fake news videos and detect fake news videos from three levels:\nSignal level, Semantic level, Intent level. Signal Level Fake news videos often contain manipulated or generated video and audio content. The manipulated or generated video and audio content refers to the following two operations:\nEditing: visual alterations on existing data of video and audio modality. Generating: generating actions are done by neural networks which are trained to directly generate complete vivid videos [2] or texts [3]. Since fake news videos might be created by using forgery techniques (i.e., editing or generating), the detection of video forgery traces would provide a significant clue for detecting a fake news video.\nHowever, through signal level clues provide strong evidence, they are not decisive because of the wide use of portable editing tools. Even if editing or generation traces are detected, it does not necessarily mean that the video is a fake news video.\nSemantic Level Falsehood is conveyed through incorrect semantic changes that are against the truth. The semantic level fake news video usually use an unaltered video in a new but false or misleading context. In other words, the creator may upload a real video of an event that happened before but add a real text description of a newly emerging event. The semantic level fake news video is also known as out-of-context (OOC) fake news video.\nThe above Fig. is an example of OOC fake news in the form of image-text in [4]. The capation of the figure is:\nNick Clegg (left) in Oldham with Liberal Demorcrat candidate Elwyn Watkins (centre) and Lib Dem Deputy leader Simon Hughes.\nDetecting the OOC fake news video poses a unique challenge since the visual content remains authentic, and the deception stems solely from the context created by combining these visual content with misleading or incorrect text.\nMany recent works focus on leveraging multi-modal semantic clues from not only the video content but also descriptive textual informaiton.\nTextual Information There are studies using the textual information to detect the fake news videos:\nusing the directly accessible texts, i.e., video description and title. using the indirectly accessible texts, i.e., subtitles and transcriptions extracted from the video. Shang et al. [5] and Choi and Ko [6] use the bidirectional recurrent neural networks to encode the textual information (title, description, and transcription) into the numerical feature presentation. The problem of these studies is that the do not really consider the visual features. These methods will fail when dealing with fake news videos that lack textual information.\nVisual Information There are studies using the visual feature to detect the fake news videos.\nFrame level: extracting the static visual features. Clip level: extracting the static visual features and temporal features. Example Studies:\nWant et al. [7] break the video into clips with fixed duration and uses S3D to extract visual features. Liu, Yacoob, and Shrivastava [8] encode frames into features using the pre-trained vision transformer ViT. It is note that consider the video itself presents the technical challenge.\nUsing tranditional two-stream models, such as S3D, to extract features from videos requires the extraction of optical flow features. Extracting optical flow features is very time-consuming, which makes the training process extremely lengthy. Moreover, even after the model has been trained, extracting optical flow features from test videos during the inference process also consumes a significant amount of time (following Fig. is an example of optical flow feature). Existing vision transformers can only extract features from videos lasting up to 10 seconds, while news videos often contain clips longer than one minute. This makes it difficult to use vision transformers to extract features from news videos. Audio Information Audio is a unique modality compared to other studies that only consider text-image fake news detection. The audio modality includes speech, environment sound, and background music.\nExample Study:\nQi et al., [4] uses the pre-trained VGGish to extract the audio features. Intent Level The creation of fake news is often motivated by underlying intents, such as political influence, financial gain, and propaganda effects [1].\nTo achieve the underlying intent, fake news videos generally pursue wide and fast spread. This leads to unique patterns of\nexpression, propagation, and user feedback, which are different from the real news videos [9] [4] [10]. Current studies mostly make use of user comments and statistics on user engagement.\nThe comments are usually exploited by extracting hand-crafted features or generating numerical representations through deep models. The user engagement includes the number of likes and views. Example Study:\nChoi and Ko [[6]] (#6) generate video comment embeddings by calculating the weighted sum of embeddings of each comment using their numbers of likes. The problem of the existing studies is that they largely ignore or oversimplify the video propagation information which however has been shown conducive to providing useful clues for identifying fake news in the text-based fake news detection methods (see following Fig.).\nMulti-modal Fusion Existing studies generally combine multiple features from different modalities to make judgment.\nFeature level fusion: features with different modalities are fused before being input into the final classification layer. Decision level fusion: predictions are produced independently by different branches and combined using strategies like voting for final decisions. We only discuss the feature level fusion in the following.\nConcatenation-Based:\nEach modality is embedded into a representation vector and then concatenated as a multi-modal representation. Attention-Based\nShang et al. [[5]] (#5) use a co-attention module that simultaneously learns the pairwise relation between each pair of a video frame and spoken word to fuse the visual and speech information. Qi et al. [[9]] (#9) utilize a cross-modal transformer to model the multual interaction between different modalities. References [1] Bu et al., Combating online misinformation videos: Characterization, detection, and future directions. In Proc. ACM MM, 2023.\n[2] Chan et al., Everbody dance now, in Proc. ICCV, 2019.\n[3] Singer et al., Make-a-video: Text-to-video generation without text-video data, in Proc. ICLR, 2023.\n[4] Qi et al., SNIFFER: Multimodal large language model for explainable out-of-context misinformation detection, in Proc. CVPR, 2024.\n[5] Shang et al., A multimodal misinformation detector for covid-19 short videos on tiktok, in Proc. IEEE Big Data, 2021.\n[6] Choi and Ko., Using topic modeling and adversarial neural networks for fake news video detection, in Proc. ACM CIKM, 2021.\n[7] Wang et al., Misinformation detection in social media video posts, in arXiv, 2022.\n[8] Fuxiao Liu, Yaser Yacoob, and Abhinav Shrivastava., Covid-vts: Fact extraction and verification on short video platforms, in Proc. EACL, 2023.\n[9] Qi et al., Fakesv: A multimodal benchmark with rich social context for fake news detection on short video platforms, in Proc. AAAI, 2023.\n[10] Yuan et al., Jointly embedding the local and global relations for heterogeneous graph for rumor detection, in Proc. ICDM, 2019.\n","wordCount":"1145","inLanguage":"en","image":"https://andr2w.github.io/COMM7960GroupProject/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://andr2w.github.io/COMM7960GroupProject/related-works/"},"publisher":{"@type":"Organization","name":"COMM7960 AI for Interactive Media Design Group Project - Yifeng LUO","logo":{"@type":"ImageObject","url":"https://andr2w.github.io/COMM7960GroupProject/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://andr2w.github.io/COMM7960GroupProject/ accesskey=h title="Home (Alt + H)"><img src=https://andr2w.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://andr2w.github.io/COMM7960GroupProject/introduction/ title=Introduction><span>Introduction</span></a></li><li><a href=https://andr2w.github.io/COMM7960GroupProject/problem-definition/ title="Problem Definition"><span>Problem Definition</span></a></li><li><a href=https://andr2w.github.io/COMM7960GroupProject/related-works/ title="Related Works"><span class=active>Related Works</span></a></li><li><a href=https://andr2w.github.io/COMM7960GroupProject/methodology/ title=Methodology><span>Methodology</span></a></li><li><a href=https://andr2w.github.io/COMM7960GroupProject/experiment/ title=Pre-Experiment><span>Pre-Experiment</span></a></li><li><a href=https://andr2w.github.io/COMM7960GroupProject/conclusion/ title=Conclusion><span>Conclusion</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://andr2w.github.io/COMM7960GroupProject/>Home</a></div><h1 class="post-title entry-hint-parent">Related Works</h1><div class=post-meta>6 min&nbsp;·&nbsp;1145 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/Related%20Works/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>To detect fake news videos, a better approach is to first understand how the fake news video is produced.
Existing literature <a href=#1>[1]</a> analysis fake news videos and detect fake news videos from three levels:</p><ul><li>Signal level,</li><li>Semantic level,</li><li>Intent level.</li></ul><h2 id=signal-level>Signal Level<a hidden class=anchor aria-hidden=true href=#signal-level>#</a></h2><p>Fake news videos often contain manipulated or generated video and audio content. The manipulated or generated video and audio content refers to the following two operations:</p><ul><li>Editing: visual alterations on existing data of video and audio modality.</li><li>Generating: generating actions are done by neural networks which are trained to directly generate complete vivid videos <a href=#2>[2]</a> or texts <a href=#3>[3]</a>.</li></ul><p>Since fake news videos might be created by using forgery techniques (i.e., editing or generating), the detection of video forgery traces would provide a significant clue for detecting a fake news video.</p><p>However, through signal level clues provide strong evidence, they are not decisive because of the wide use of portable editing tools. Even if editing or generation traces are detected, it does not necessarily mean that the video is a fake news video.</p><h2 id=semantic-level>Semantic Level<a hidden class=anchor aria-hidden=true href=#semantic-level>#</a></h2><p>Falsehood is conveyed through incorrect semantic changes that are against the truth. The semantic level fake news video usually use an unaltered video in a new but false or misleading context. In other words, the creator may upload a real video of an event that happened before but add a real text description of a newly emerging event. The semantic level fake news video is also known as out-of-context (<strong>OOC</strong>) fake news video.</p><p><img loading=lazy src=1.png alt="Figure 1: Example of Fake News Video"></p><p>The above Fig. is an example of OOC fake news in the form of image-text in <a href=#4>[4]</a>. The capation of the figure is:</p><blockquote><p>Nick Clegg (left) in Oldham with Liberal Demorcrat candidate Elwyn Watkins (centre) and Lib Dem Deputy leader Simon Hughes.</p></blockquote><p>Detecting the OOC fake news video poses a unique challenge since the visual content remains authentic, and the deception stems solely from the context created by combining these visual content with misleading or incorrect text.</p><p>Many recent works focus on leveraging multi-modal semantic clues from not only the video content but also descriptive textual informaiton.</p><h3 id=textual-information>Textual Information<a hidden class=anchor aria-hidden=true href=#textual-information>#</a></h3><p>There are studies using the textual information to detect the fake news videos:</p><ul><li>using the directly accessible texts, i.e., video description and title.</li><li>using the indirectly accessible texts, i.e., subtitles and transcriptions extracted from the video.</li></ul><p>Shang et al. <a href=#5>[5]</a> and Choi and Ko <a href=#6>[6]</a> use the bidirectional recurrent neural networks to encode the textual information (title, description, and transcription) into the numerical feature presentation. <strong>The problem</strong> of these studies is that the do not <strong>really</strong> consider the visual features. These methods will fail when dealing with fake news videos that lack textual information.</p><h3 id=visual-information>Visual Information<a hidden class=anchor aria-hidden=true href=#visual-information>#</a></h3><p>There are studies using the visual feature to detect the fake news videos.</p><ul><li>Frame level: extracting the static visual features.</li><li>Clip level: extracting the static visual features and <strong>temporal features</strong>.</li></ul><p>Example Studies:</p><ul><li>Want et al. <a href=#7>[7]</a> break the video into clips with fixed duration and uses S3D to extract visual features.</li><li>Liu, Yacoob, and Shrivastava <a href=#8>[8]</a> encode frames into features using the pre-trained vision transformer ViT.</li></ul><p><strong>It is note that consider the video itself presents the technical challenge.</strong></p><ul><li>Using tranditional two-stream models, such as S3D, to extract features from videos requires the extraction of optical flow features. Extracting optical flow features is very time-consuming, which makes the training process extremely lengthy. Moreover, even after the model has been trained, extracting optical flow features from test videos during the inference process also consumes a significant amount of time (following Fig. is an example of optical flow feature).</li></ul><p><img loading=lazy src=2.png alt=image></p><ul><li>Existing vision transformers can only extract features from videos lasting up to 10 seconds, while news videos often contain clips longer than one minute. This makes it difficult to use vision transformers to extract features from news videos.</li></ul><h3 id=audio-information>Audio Information<a hidden class=anchor aria-hidden=true href=#audio-information>#</a></h3><p>Audio is a unique modality compared to other studies that only consider text-image fake news detection. The audio modality includes speech, environment sound, and background music.</p><p>Example Study:</p><ul><li>Qi et al., <a href=#4>[4]</a> uses the pre-trained VGGish to extract the audio features.</li></ul><h2 id=intent-level>Intent Level<a hidden class=anchor aria-hidden=true href=#intent-level>#</a></h2><p>The creation of fake news is often motivated by underlying intents, such as political influence, financial gain, and propaganda effects <a href=#1>[1]</a>.</p><p>To achieve the underlying intent, fake news videos generally pursue wide and fast spread. This leads to unique patterns of</p><ul><li>expression,</li><li>propagation,</li><li>and user feedback,
which are different from the real news videos <a href=#9>[9]</a> <a href=#4>[4]</a> <a href=#10>[10]</a>.</li></ul><p>Current studies mostly make use of user comments and statistics on user engagement.</p><ul><li>The comments are usually exploited by extracting hand-crafted features or generating numerical representations through deep models.</li><li>The user engagement includes the number of likes and views.</li></ul><p>Example Study:</p><ul><li>Choi and Ko [[6]] (#6) generate video comment embeddings by calculating the weighted sum of embeddings of each comment using their numbers of likes.</li></ul><p>The problem of the existing studies is that they largely ignore or oversimplify the video propagation information which however has been shown conducive to providing useful clues for identifying fake news in the text-based fake news detection methods (see following Fig.).</p><p><img loading=lazy src=3.png alt=image></p><h3 id=multi-modal-fusion>Multi-modal Fusion<a hidden class=anchor aria-hidden=true href=#multi-modal-fusion>#</a></h3><p>Existing studies generally combine multiple features from different modalities to make judgment.</p><ul><li>Feature level fusion: features with different modalities are fused before being input into the final classification layer.</li><li>Decision level fusion: predictions are produced independently by different branches and combined using strategies like voting for final decisions.</li></ul><p>We only discuss the feature level fusion in the following.</p><p><strong>Concatenation-Based</strong>:</p><ul><li>Each modality is embedded into a representation vector and then concatenated as a multi-modal representation.</li></ul><p><strong>Attention-Based</strong></p><ul><li>Shang et al. [[5]] (#5) use a co-attention module that simultaneously learns the pairwise relation between each pair of a video frame and spoken word to fuse the visual and speech information.</li><li>Qi et al. [[9]] (#9) utilize a cross-modal transformer to model the multual interaction between different modalities.</li></ul><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Bu et al., Combating online misinformation videos: Characterization, detection, and future directions. In Proc. ACM MM, 2023.</p><p>[2] Chan et al., Everbody dance now, in Proc. ICCV, 2019.</p><p>[3] Singer et al., Make-a-video: Text-to-video generation without text-video data, in Proc. ICLR, 2023.</p><p>[4] Qi et al., SNIFFER: Multimodal large language model for explainable out-of-context misinformation detection, in Proc. CVPR, 2024.</p><p>[5] Shang et al., A multimodal misinformation detector for covid-19 short videos on tiktok, in Proc. IEEE Big Data, 2021.</p><p>[6] Choi and Ko., Using topic modeling and adversarial neural networks for fake news video detection, in Proc. ACM CIKM, 2021.</p><p>[7] Wang et al., Misinformation detection in social media video posts, in arXiv, 2022.</p><p>[8] Fuxiao Liu, Yaser Yacoob, and Abhinav Shrivastava., Covid-vts: Fact extraction and verification on short video platforms, in Proc. EACL, 2023.</p><p>[9] Qi et al., Fakesv: A multimodal benchmark with rich social context for fake news detection on short video platforms, in Proc. AAAI, 2023.</p><p>[10] Yuan et al., Jointly embedding the local and global relations for heterogeneous graph for rumor detection, in Proc. ICDM, 2019.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://andr2w.github.io/COMM7960GroupProject/>COMM7960 AI for Interactive Media Design Group Project - Yifeng LUO</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>